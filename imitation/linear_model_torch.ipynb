{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('valid_100K_disc1_cleaned.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((95623, 11, 11, 18), (95623,), (95623,))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x_train = data['observations']\n",
    "p_train = data['actions']\n",
    "v_train = data['rewards']\n",
    "x_train.shape, p_train.shape, v_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size=11*11*18, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc0  = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_p = nn.Linear(hidden_size, 6)\n",
    "        self.fc_v = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1).float()\n",
    "        x = F.relu(self.fc0(x))\n",
    "        p = F.softmax(self.fc_p(x), dim=-1)\n",
    "        v = torch.tanh(self.fc_v(x))\n",
    "        return p, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(hidden_size=512)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 00 - Loss = 2.964 - Loss_p = 1.964 - Loss_v = 1.000\n",
      "Epoch 01 - Loss = 2.554 - Loss_p = 1.874 - Loss_v = 0.680\n",
      "Epoch 02 - Loss = 2.804 - Loss_p = 1.884 - Loss_v = 0.920\n",
      "Epoch 03 - Loss = 2.924 - Loss_p = 1.804 - Loss_v = 1.120\n",
      "Epoch 04 - Loss = 2.974 - Loss_p = 1.894 - Loss_v = 1.080\n",
      "Epoch 05 - Loss = 2.794 - Loss_p = 1.874 - Loss_v = 0.920\n",
      "Epoch 06 - Loss = 2.884 - Loss_p = 1.924 - Loss_v = 0.960\n",
      "Epoch 07 - Loss = 3.124 - Loss_p = 1.924 - Loss_v = 1.200\n",
      "Epoch 08 - Loss = 2.984 - Loss_p = 1.824 - Loss_v = 1.160\n",
      "Epoch 09 - Loss = 3.034 - Loss_p = 1.914 - Loss_v = 1.120\n",
      "Epoch 10 - Loss = 2.594 - Loss_p = 1.914 - Loss_v = 0.680\n",
      "Epoch 11 - Loss = 3.014 - Loss_p = 1.894 - Loss_v = 1.120\n",
      "Epoch 12 - Loss = 2.944 - Loss_p = 1.864 - Loss_v = 1.080\n",
      "Epoch 13 - Loss = 3.094 - Loss_p = 1.934 - Loss_v = 1.160\n",
      "Epoch 14 - Loss = 2.934 - Loss_p = 1.854 - Loss_v = 1.080\n",
      "Epoch 15 - Loss = 2.794 - Loss_p = 1.914 - Loss_v = 0.880\n",
      "Epoch 16 - Loss = 2.774 - Loss_p = 1.934 - Loss_v = 0.840\n",
      "Epoch 17 - Loss = 2.804 - Loss_p = 1.884 - Loss_v = 0.920\n",
      "Epoch 18 - Loss = 2.744 - Loss_p = 1.824 - Loss_v = 0.920\n",
      "Epoch 19 - Loss = 2.774 - Loss_p = 1.934 - Loss_v = 0.840\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_idx = np.random.randint(0, len(p_train), size=batch_size)\n",
    "    x = torch.tensor(x_train[batch_idx])\n",
    "    p = torch.tensor(p_train[batch_idx])\n",
    "    v = torch.tensor(v_train[batch_idx])\n",
    "\n",
    "    p_pred, v_pred = model(x)\n",
    "    loss_p = F.cross_entropy(p_pred, p)\n",
    "    loss_v = torch.mean((v - v_pred)**2)\n",
    "    loss = loss_p + loss_v\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} - Loss = {loss.item():-5.3f} - Loss_p = {loss_p.item():05.3f} - Loss_v = {loss_v.item():05.3f}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9ad8c1dff7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.mean((predictions[1].squeeze() - torch.tensor(v_train[batch_idx]))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6862, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(predictions[1].squeeze(), torch.tensor(v_train[batch_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "torch.tensor(v_train[batch_idx], dtype=torch.float).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "predictions[1].squeeze().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "loss.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.6.6 64-bit ('.venv')",
   "display_name": "Python 3.6.6 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "18ee0daf271ee8587ea6c34d4d183bf741ccbce31dcaede1a5f6373776bcfec0"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}